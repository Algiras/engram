---
title: "Hybrid Retrieval for Persistent Developer Memory"
subtitle: "How combining HyDE, BM25 hybrid search, and multi-category extraction beats single-technique baselines"
date: today
---

```{python}
%matplotlib inline
import json, pathlib, warnings
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns
import numpy as np
from IPython.display import display

warnings.filterwarnings("ignore")
plt.rcParams["figure.dpi"] = 120

# ── styling ────────────────────────────────────────────────────────────────
plt.rcParams.update({
    "figure.facecolor": "white",
    "axes.facecolor":   "white",
    "axes.spines.top":  False,
    "axes.spines.right": False,
    "font.family":      "sans-serif",
    "axes.grid":        True,
    "grid.alpha":       0.25,
    "grid.linestyle":   "--",
})

BLUE   = "#0f3460"
RED    = "#e94560"
GOLD   = "#f7c59f"
GREY   = "#aab4c8"
GREEN  = "#1a6b35"
LIGHT  = "#d0d4de"

# ── data loading ───────────────────────────────────────────────────────────
BASE = pathlib.Path("../../")

def load(name):
    p = BASE / "eval" / f"domain_results_{name}.json"
    d = json.loads(p.read_text())
    cats = d.get("by_category", {})
    total_n = d.get("total_questions") or sum(
        v.get("n", 0) for v in cats.values() if isinstance(v, dict)
    )
    return dict(
        f1    = float(d.get("overall_f1") or d.get("f1", 0)),
        judge = float(d.get("overall_judge") or d.get("judge_score", 0)),
        nfr   = d.get("not_found_rate"),
        n     = total_n,
        cats  = cats,
    )

V = {
    "Baseline": load("baseline"),
    "v1":       load("v1"),
    "v2":       load("v2"),
    "v3":       load("v3"),
    "v3-full":  load("v3_full"),
    "v4":       load("v4"),
    "v5":       load("v5"),
    "v6":       load("v6"),
    "v6b":      load("v6b"),
    "Ceiling":  load("ceiling"),
}
CATS = ["decisions", "solutions", "patterns", "bugs", "insights", "procedures"]
```

::: {.hero}
::: {.hero-left}
## Results at a glance

386 domain questions · 6 knowledge categories · two metrics:
**Token F1** (deterministic lexical overlap) and **LLM Judge**
(Gemini 2.5 Flash, binary semantic correctness × 100).
Ceiling = full-context upper bound with no retrieval step.
:::
::: {.hero-right}
<div class="kpi accent"><div class="kpi-val">60.5</div><div class="kpi-lbl">Token F1</div></div>
<div class="kpi accent"><div class="kpi-val">16.3</div><div class="kpi-lbl">LLM Judge</div></div>
<div class="kpi"><div class="kpi-val">115%</div><div class="kpi-lbl">of F1 ceiling</div></div>
<div class="kpi"><div class="kpi-val">115%</div><div class="kpi-lbl">of Judge ceiling</div></div>
<div class="kpi"><div class="kpi-val">13.2%</div><div class="kpi-lbl">Not-found rate</div></div>
:::
:::

---

## The problem: retrieval is harder than it looks

Developers accumulate knowledge in structured notes — decisions, solutions, patterns, bugs, insights, procedures. Retrieving the right entry when asked a natural-language question is a deceptively hard retrieval problem:

- **Vocabulary gap.** A question like *"how do we handle async?"* shares few tokens with the stored answer *"We decided to use tokio for all async operations in CLI tools."*
- **Exact-term failure.** Dense embeddings are good at semantic similarity but silently miss queries that hinge on a specific function name, version number, or error code.
- **Category pollution.** A single conversation contains decisions, workarounds, and code patterns. Routing everything to one bucket causes retrieval to compete across unrelated topics.

This report shows how three techniques address these failure modes — and documents the measured contribution of each.

---

## Benchmark setup

| | |
|---|---|
| **Dataset** | 386 QA pairs hand-authored across 6 categories |
| **Retriever** | engram — embeds knowledge entries offline, retrieves at query time |
| **Answering model** | Claude claude-haiku-4-5 (same for all versions) |
| **Metrics** | Token F1 (precision/recall over tokens) · LLM Judge (0 or 1 per question × 100) |
| **Ceiling** | Full knowledge base injected as context — upper bound for a perfect retriever |

**Why two metrics?** Token F1 is deterministic and rewards exact overlap — good for factual recall but blind to paraphrases. The LLM judge catches semantically correct answers that use different wording. Together they bracket retrieval quality from two sides.

**Why the ceiling matters.** The ceiling is not achievable in practice (it requires unlimited context), but it tells us how much room retrieval leaves on the table.

---

## Landscape comparison

The persistent memory space has several active systems. None publish results on the same benchmark or metric as this evaluation, so the table below compares **architecture and retrieval design** rather than numbers.

| System | Storage model | Retrieval technique | Knowledge structure | Open benchmark |
|---|---|---|---|---|
| **engram (v6b)** | Flat markdown files per category | HyDE + BM25 hybrid + RRF | 6 domain categories (decisions, solutions, patterns, bugs, insights, procedures) | ✅ this report |
| **Mem0** | Vector store + entity graph | Dense semantic search | Flat fact extraction, entity-linked | ❌ proprietary LOCOMO subset |
| **MemGPT / Letta** | Hierarchical paged context | In-context paging + retrieval | Conversation segments | ❌ internal only |
| **Zep** | Graph + vector hybrid | Graph-guided semantic search | Entity/relationship extraction | ❌ no public dataset |
| **Naive RAG** | Vector store (flat) | Dense cosine only | No structure | varies |

**Key architectural differences vs dense-only systems:**

- *Category-aware extraction* — engram routes each insight to a typed bucket at write time. At retrieval time, the search space is smaller and topic-coherent; competing with an unsorted flat store is a known source of retrieval noise.
- *HyDE* — most deployed systems embed the raw query. Generating a hypothetical answer first and embedding *that* consistently outperforms raw-query embedding on knowledge-base QA (Gao et al., 2023), especially when question and answer phrasing diverge.
- *BM25 hybrid* — Mem0 and Zep use pure vector search. Exact-term queries (function names, version strings, error codes) are systematically underserved by dense retrieval alone; BM25 hybrid closes that gap.

::: {.callout-note}
**Reproducibility.** The 386-question evaluation dataset is available in `eval/domain_*.json`. Other systems can run the same questions against their retrieval stack and report Token F1 and LLM Judge for a direct comparison.
:::

---

## How each technique contributes

```{python}
#| fig-cap: "F1 and judge scores across all versions. Dashed lines = ceiling upper bounds."
#| out-width: 100%

labels = list(V.keys())
f1s    = [v["f1"]    for v in V.values()]
judges = [v["judge"] for v in V.values()]
cf1    = V["Ceiling"]["f1"]
cj     = V["Ceiling"]["judge"]
x      = np.arange(len(labels))

fig, ax = plt.subplots(figsize=(10, 4))

ax.axhspan(cf1 - 1, cf1 + 1, alpha=0.08, color=BLUE,  zorder=0)
ax.axhspan(cj  - 1, cj  + 1, alpha=0.08, color=RED,   zorder=0)
ax.axhline(cf1, color=BLUE, lw=1.4, ls="--", alpha=0.6, label=f"F1 ceiling ({cf1:.1f})")
ax.axhline(cj,  color=RED,  lw=1.4, ls=":",  alpha=0.6, label=f"Judge ceiling ({cj:.1f})")

sizes = [120 if l in ("v6b", "Baseline") else 55 for l in labels]

ax.plot(x, f1s,    color=BLUE, lw=2.5, marker="o", ms=7, label="Token F1",  zorder=3)
ax.plot(x, judges, color=RED,  lw=2.5, marker="s", ms=7, label="LLM Judge", zorder=3)

# highlight v6b
v6b_idx = labels.index("v6b")
ax.scatter([v6b_idx], [f1s[v6b_idx]],    s=160, color=BLUE, zorder=4)
ax.scatter([v6b_idx], [judges[v6b_idx]], s=160, color=RED,  zorder=4)

ax.set_xticks(x)
ax.set_xticklabels(labels, fontsize=10)
ax.set_ylim(0, 80)
ax.set_ylabel("Score", fontsize=11)
ax.legend(loc="upper left", fontsize=10, framealpha=0.9)
fig.tight_layout()
display(fig)
plt.close(fig)
```

### Technique 1 — HyDE: bridging the vocabulary gap (+14.9 F1)

**v2 → v3** is the largest single jump in the chart.

Dense embedding models embed short questions and long knowledge entries in the same vector space — but they land far apart because they look nothing alike at the token level. **Hypothetical Document Embedding (HyDE)** sidesteps this by generating a plausible hypothetical answer *before* searching:

```
Question: "what did we decide about async runtimes?"

Hypothetical answer (generated, not stored):
  "We decided to use tokio as the async runtime for CLI tools
   that require concurrent LLM calls, because it provides
   a mature ecosystem and integrates well with reqwest."

embed(hypothetical) → cosine search → top-k real entries → answer
```

The hypothetical answer lives in the same region of embedding space as real answers. The vocabulary gap disappears because we are now searching *answer space → answer space* instead of *question space → answer space*.

**Impact:** +14.9 F1 (v2=37.6 → v3=52.5). The gain is largest for insights and decisions — categories where phrasing between question and answer diverges the most.

### Technique 2 — Multi-category extraction: eliminating category pollution

The same commit that introduced HyDE also changed how knowledge is extracted. Previously each conversation was routed to a *single* category bucket. Now the extraction LLM assigns each insight to whichever of the six categories it belongs to — decisions, solutions, patterns, bugs, insights, procedures — simultaneously.

**Why it matters for retrieval:** with per-conversation buckets, a conversation about a bug fix and a design decision would compete in the same pool. With category-level routing, the retriever can search the correct pool first, reducing irrelevant candidates.

### Technique 3 — BM25 hybrid with RRF: rescuing exact-term queries (+3.3 F1)

Dense retrieval is good at *semantic* similarity. It struggles when the query is exact: version numbers (`v0.3.1`), error codes (`ECONNREFUSED`), function names (`parse_session_block`). BM25 is purely lexical — it excels at exactly the queries dense search misses.

**Reciprocal Rank Fusion** merges both ranked lists without requiring score calibration:

$$\text{score}(d) = \frac{1}{60 + r_{\text{dense}}(d)} + \frac{1}{60 + r_{\text{BM25}}(d)}$$

The constant 60 dampens rank sensitivity, making the fusion robust to small positional differences at the top. Standard BM25 parameters: $k_1 = 1.5$, $b = 0.75$.

```{python}
#| fig-cap: "F1 change at each version. Most gains come from v2→v3 (HyDE) and v5→v6 (BM25 hybrid)."
#| out-width: 100%

prev   = [None] + f1s[:-1]
deltas = [round(f - p, 1) if p else 0.0 for f, p in zip(f1s, prev)]

bar_labels = labels[1:]
bar_vals   = deltas[1:]
bar_colors = [GREEN if d > 0.5 else "#9b1c1c" if d < -0.5 else GREY for d in bar_vals]

fig, ax = plt.subplots(figsize=(10, 3.5))
bars = ax.bar(bar_labels, bar_vals, color=bar_colors, width=0.55, zorder=3)
ax.axhline(0, color="#999", lw=1)

for bar, val in zip(bars, bar_vals):
    offset = 0.3 if val >= 0 else -0.8
    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + offset,
            f"{val:+.1f}", ha="center", va="bottom" if val >= 0 else "top",
            fontsize=9, fontweight="bold")

ax.set_ylabel("ΔF1 vs previous version", fontsize=11)
ax.set_ylim(-4, 18)
fig.tight_layout()
display(fig)
plt.close(fig)
```

**v5 → v6 also bundled two dataset fixes** that are worth separating from the retrieval improvement:

- **Procedures QA regenerated.** The original 51 questions referenced procedures by ordinal position ("the first procedure") — retrieval is a lottery when questions carry no distinctive terms. Regenerated with named references, making the dataset tractable.
- **ADD artifact fixed.** The extraction LLM occasionally prefixed entries with resolver keywords (`ADD`, `NOOP`), polluting knowledge files. Root cause fixed; the prefix no longer appears.

---

## Category-level analysis

```{python}
#| fig-cap: "Token F1 by category for four reference points: baseline, v3-full (HyDE only), v6b (full stack), ceiling."
#| out-width: 100%

run_cfg = [
    ("Baseline", LIGHT,  "//"),
    ("v3-full",  "#6ea8fe", None),
    ("v6b",      RED,     None),
    ("Ceiling",  GREY,    ".."),
]

cat_vals = {
    run: [V[run]["cats"].get(c, {}).get("f1", 0) for c in CATS]
    for run, *_ in run_cfg
}

x   = np.arange(len(CATS))
w   = 0.18
fig, ax = plt.subplots(figsize=(11, 4.5))

for i, (run, color, hatch) in enumerate(run_cfg):
    offset = (i - 1.5) * w
    bars = ax.bar(x + offset, cat_vals[run], width=w, color=color,
                  hatch=hatch, label=run, zorder=3,
                  edgecolor="white", linewidth=0.8)
    for bar in bars:
        h = bar.get_height()
        if h > 5:
            ax.text(bar.get_x() + bar.get_width() / 2, h + 0.8,
                    f"{h:.0f}", ha="center", va="bottom", fontsize=7.5)

ax.set_xticks(x)
ax.set_xticklabels(CATS, fontsize=11)
ax.set_ylim(0, 100)
ax.set_ylabel("Token F1", fontsize=11)
ax.legend(fontsize=10, framealpha=0.9)
fig.tight_layout()
display(fig)
plt.close(fig)
```

**Key observations:**

- **Procedures** gained the most (+28 F1, baseline → v6b). Almost entirely from the QA dataset fix — once questions were answerable, retrieval performed well.
- **Decisions** and **solutions** are closest to ceiling. Well-structured factual entries are largely solved at the current corpus size.
- **Bugs** and **patterns** have the largest remaining gaps (−11 and −10 vs ceiling). Bug reports have high phrasing variation around technical identifiers; pattern descriptions use abstract vocabulary where synonym variation suppresses token F1 even when the semantic answer is correct.
- **Insights** scores highest on the judge metric (25.8) despite moderate F1 (50.3) — the LLM finds answers semantically correct even when exact wording differs.

---

## Not-found rate: what BM25 fixed

```{python}
#| fig-cap: "Fraction of questions where the retriever returned no answer. BM25 hybrid reduced this by ~5 points."
#| out-width: 100%

versions_nfr = {k: v for k, v in V.items() if v["nfr"] is not None and k != "Ceiling"}
nfr_labels   = list(versions_nfr.keys())
nfr_vals     = [v["nfr"] for v in versions_nfr.values()]
nfr_colors   = [RED if n < 15 else BLUE for n in nfr_vals]

fig, ax = plt.subplots(figsize=(10, 3.2))
bars = ax.bar(nfr_labels, nfr_vals, color=nfr_colors, width=0.5, zorder=3)

for bar, val in zip(bars, nfr_vals):
    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.3,
            f"{val:.1f}%", ha="center", va="bottom", fontsize=9)

ax.set_ylabel("Questions with no retrieved answer (%)", fontsize=11)
ax.set_ylim(0, 25)
fig.tight_layout()
display(fig)
plt.close(fig)
```

Before BM25, 18.4% of queries returned nothing — the dense index had no match above the similarity threshold. After adding BM25 hybrid, the not-found rate dropped to **13.2%**, recovering ~20 additional questions per run. These tend to be exact-term queries: specific function names, error messages, version strings.

---

## Full results table

```{python}
rows = []
for name, v in V.items():
    pf1 = f"{v['f1']/V['Ceiling']['f1']*100:.0f}%" if name != "Ceiling" else "—"
    pj  = f"{v['judge']/V['Ceiling']['judge']*100:.0f}%" if name != "Ceiling" else "—"
    nfr = f"{v['nfr']:.1f}%" if v["nfr"] is not None else "—"
    rows.append({
        "Version":   name,
        "Token F1":  f"{v['f1']:.1f}",
        "LLM Judge": f"{v['judge']:.1f}",
        "% F1 ceil": pf1,
        "% J ceil":  pj,
        "Not-found": nfr,
        "N":         str(int(v["n"])) if v["n"] else "—",
    })

df = pd.DataFrame(rows)
display(df.style
    .set_properties(**{"text-align": "center"})
    .set_properties(subset=["Version"], **{"text-align": "left", "font-weight": "bold"})
    .apply(lambda x: ["background: #fff8e7; font-weight:600" if v == "v6b"
                       else "color:#aaa; font-style:italic" if v == "Ceiling"
                       else "" for v in x], subset=["Version"])
    .hide(axis="index")
)
```

---

## Remaining gaps and next steps

```{python}
#| fig-cap: "Gap between v6b and ceiling per category. Negative = still below ceiling."
#| out-width: 100%

gap = {
    c: V["v6b"]["cats"].get(c, {}).get("f1", 0) - V["Ceiling"]["cats"].get(c, {}).get("f1", 0)
    for c in CATS
}

fig, ax = plt.subplots(figsize=(9, 3.5))
colors  = [GREEN if v >= 0 else "#9b1c1c" for v in gap.values()]
bars    = ax.bar(CATS, gap.values(), color=colors, width=0.5, zorder=3)
ax.axhline(0, color="#333", lw=1)

for bar, val in zip(bars, gap.values()):
    offset = 0.3 if val >= 0 else -1.0
    ax.text(bar.get_x() + bar.get_width() / 2, val + offset,
            f"{val:+.1f}", ha="center", va="bottom" if val >= 0 else "top",
            fontsize=9, fontweight="bold")

ax.set_ylabel("v6b F1 − Ceiling F1", fontsize=11)
ax.set_title("Remaining gap to ceiling (positive = surpassed ceiling)", fontsize=11)
fig.tight_layout()
display(fig)
plt.close(fig)
```

**Bugs and patterns** have the largest gaps (~11 pts each). Both categories have high surface-form variation — the semantic content is often correct but exact token overlap is low. Two directions that address this:

1. **Cross-encoder re-ranking.** After retrieving top-k candidates, a small cross-encoder re-scores them using full pairwise attention. Unlike bi-encoder similarity, cross-encoders see both query and candidate together, making them much better at semantic relevance than token overlap.

2. **Query expansion.** Generate multiple query variants from the original question and merge result lists with RRF. Particularly useful for short, ambiguous bug queries where a single embedding may land in the wrong region.

---

## Metric notes

**Why judge can exceed ceiling.** The ceiling injects the *entire* knowledge base as context — thousands of tokens of mixed content. The model has to locate and extract the relevant fragment from noise. RAG retrieves a small, focused set; the model sees less noise and gives cleaner answers that the judge scores higher. This is a known property of RAG vs long-context injection and is not a measurement artefact.

**Judge variance.** Binary scoring (0/1 per question × 100) means each question contributes `100/N ≈ 0.26` pts. A swing of ±10 questions changes the score by ±2.6 pts. Two identical runs can differ by 2–3 pts. The CI regression gate is set at 2.5 pts to account for this; v6 and v6b were run twice to confirm reproducibility.

**Token F1 limitations.** F1 rewards token overlap but penalises paraphrases. A semantically correct answer with different wording scores lower than a partially wrong answer that reproduces the reference verbatim. This is why insights shows F1=50.3 but judge=25.8 — the answers are semantically right but worded differently from the reference.
