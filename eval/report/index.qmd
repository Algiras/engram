---
title: "engram ‚Äî Evaluation Report"
subtitle: "Domain knowledge retrieval: progress from baseline to SOTA"
author: "engram project"
date: today
---

```{python}
#| echo: false
import json, pathlib, warnings
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
warnings.filterwarnings("ignore")

BASE = pathlib.Path("../../")

def load(name):
    p = BASE / "eval" / f"domain_results_{name}.json"
    d = json.loads(p.read_text())
    cats = d.get("by_category", {})
    return {
        "f1":    d.get("overall_f1") or d.get("f1", 0),
        "judge": d.get("overall_judge") or d.get("judge_score", 0),
        "nfr":   d.get("not_found_rate", None),
        "n":     d.get("total_questions", sum(v.get("n",0) for v in cats.values())),
        "cats":  cats,
    }

versions = {
    "Baseline":  load("baseline"),
    "v1":        load("v1"),
    "v2":        load("v2"),
    "v3":        load("v3"),
    "v3 (full)": load("v3_full"),
    "v4":        load("v4"),
    "v5":        load("v5"),
    "v6":        load("v6"),
    "v6b ‚ú¶":     load("v6b"),
    "Ceiling":   load("ceiling"),
}

ceiling_f1    = versions["Ceiling"]["f1"]
ceiling_judge = versions["Ceiling"]["judge"]
sota_f1       = versions["v6b ‚ú¶"]["f1"]
sota_judge    = versions["v6b ‚ú¶"]["judge"]
baseline_f1   = versions["Baseline"]["f1"]
```

::: {.sota-banner}
### üèÜ Current SOTA &nbsp;¬∑&nbsp; v6b

<div style="display:flex; justify-content:center; flex-wrap:wrap; gap:0.5rem; margin-top:0.8rem;">

<div class="metric-card gold">
  <div class="metric-value">60.5</div>
  <div class="metric-label">Token F1</div>
</div>

<div class="metric-card gold">
  <div class="metric-value">16.3</div>
  <div class="metric-label">LLM Judge</div>
</div>

<div class="metric-card green">
  <div class="metric-value">89%</div>
  <div class="metric-label">of F1 Ceiling</div>
</div>

<div class="metric-card green">
  <div class="metric-value">117%</div>
  <div class="metric-label">of Judge Ceiling</div>
</div>

<div class="metric-card grey">
  <div class="metric-value">13%</div>
  <div class="metric-label">Not-found Rate</div>
</div>

</div>
:::

## Overview

engram is a persistent memory system for Claude Code that extracts, stores, and retrieves
knowledge across sessions. This report tracks retrieval quality across **386 domain-specific
questions** covering decisions, solutions, patterns, bugs, insights, and procedures.

Two complementary metrics are used:

- **Token F1** ‚Äî exact-match recall of key terms (objective, fast)
- **LLM Judge** ‚Äî binary semantic correctness scored by Gemini 2.5 Flash (subjective, qualitative)

The **ceiling** is a full-context upper bound (all knowledge injected at once, no retrieval step).
Exceeding the ceiling on judge is possible because RAG retrieves focused context without noise.

---

## Progress Over Time

```{python}
#| label: fig-progress
#| fig-cap: "Token F1 and LLM Judge scores across all evaluation versions. Dashed lines mark the full-context ceiling."

labels = list(versions.keys())
f1s    = [v["f1"]    for v in versions.values()]
judges = [v["judge"] for v in versions.values()]

fig = go.Figure()

fig.add_hline(y=ceiling_f1,    line_dash="dash", line_color="#6c757d",
              annotation_text=f"F1 ceiling {ceiling_f1:.1f}", annotation_position="right")
fig.add_hline(y=ceiling_judge, line_dash="dot",  line_color="#adb5bd",
              annotation_text=f"Judge ceiling {ceiling_judge:.1f}", annotation_position="right")

fig.add_trace(go.Scatter(
    x=labels, y=f1s, name="Token F1",
    mode="lines+markers",
    line=dict(color="#0d6efd", width=2.5),
    marker=dict(size=10, symbol=["star" if l == "v6b ‚ú¶" else "circle" for l in labels],
                color=["#ffc107" if l == "v6b ‚ú¶" else "#0d6efd" for l in labels]),
))
fig.add_trace(go.Scatter(
    x=labels, y=judges, name="LLM Judge",
    mode="lines+markers",
    line=dict(color="#198754", width=2.5),
    marker=dict(size=10, symbol=["star" if l == "v6b ‚ú¶" else "circle" for l in labels],
                color=["#ffc107" if l == "v6b ‚ú¶" else "#198754" for l in labels]),
))

fig.update_layout(
    xaxis_title="Version",
    yaxis_title="Score",
    yaxis_range=[0, 75],
    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
    template="plotly_white",
    height=420,
)
fig.show()
```

```{python}
#| echo: false
# Summary table
rows = []
for name, v in versions.items():
    pct_f1    = v["f1"]    / ceiling_f1    * 100
    pct_judge = v["judge"] / ceiling_judge * 100
    nfr = f"{v['nfr']:.1f}%" if v["nfr"] is not None else "‚Äî"
    rows.append({
        "Version":       name,
        "Token F1":      f"{v['f1']:.1f}",
        "LLM Judge":     f"{v['judge']:.1f}",
        "% of F1 ceil":  f"{pct_f1:.0f}%",
        "% of J ceil":   f"{pct_judge:.0f}%",
        "Not-found":     nfr,
        "Questions":     str(int(v["n"])) if v["n"] else "‚Äî",
    })

df = pd.DataFrame(rows)
df.style.set_caption("All evaluation runs").hide(axis="index")
```

```{python}
#| label: tbl-results
#| tbl-cap: "Evaluation results across all versions"
from IPython.display import HTML

def highlight_row(row):
    if "v6b" in row["Version"]:
        return ['background-color: #fff3cd; font-weight: bold'] * len(row)
    if row["Version"] == "Ceiling":
        return ['color: #6c757d; font-style: italic'] * len(row)
    return [''] * len(row)

styled = df.style.apply(highlight_row, axis=1).hide(axis="index")
HTML(styled.to_html())
```

---

## What Drove Each Jump

```{python}
#| label: fig-delta
#| fig-cap: "Incremental F1 gain at each version relative to previous. Largest single jump: v2‚Üív3 (+14.9 pts from HyDE + categories)."

prev_f1 = [None] + f1s[:-1]
deltas  = [f - p if p else 0 for f, p in zip(f1s, prev_f1)]
colors  = ["#198754" if d >= 0 else "#dc3545" for d in deltas]

fig = go.Figure(go.Bar(
    x=labels[1:], y=deltas[1:],
    marker_color=colors[1:],
    text=[f"{d:+.1f}" for d in deltas[1:]],
    textposition="outside",
))
fig.update_layout(
    xaxis_title="Version",
    yaxis_title="ŒîF1 vs previous",
    template="plotly_white",
    height=360,
    yaxis_range=[-3, 18],
)
fig.show()
```

| Version | Key improvements |
|---------|-----------------|
| **v1**  | Initial RAG retrieval (dense embeddings only) |
| **v2**  | Concise output formatting, smoke-test tuning |
| **v3**  | HyDE (hypothetical doc embedding) + multi-category extraction |
| **v3 (full)** | Full 385-question dataset; best pre-SOTA run |
| **v4**  | Post-merge clippy fixes, fmt cleanup |
| **v5**  | Noise run ‚Äî demonstrates judge variance (~2.3 pts) |
| **v6**  | BM25 hybrid retrieval (RRF), procedures QA regenerated, ADD artifact fix |
| **v6b ‚ú¶** | Confirmed SOTA ‚Äî second independent run |

---

## Category Breakdown

```{python}
#| label: fig-categories
#| fig-cap: "Token F1 by category: baseline, pre-SOTA (v3 full), SOTA (v6b), and ceiling."

categories = ["decisions", "solutions", "patterns", "bugs", "insights", "procedures"]
runs = {
    "Baseline":  versions["Baseline"],
    "v3 (full)": versions["v3 (full)"],
    "v6b ‚ú¶ (SOTA)": versions["v6b ‚ú¶"],
    "Ceiling":   versions["Ceiling"],
}

colors_map = {
    "Baseline":       "#adb5bd",
    "v3 (full)":      "#6ea8fe",
    "v6b ‚ú¶ (SOTA)":   "#ffc107",
    "Ceiling":        "#d3d3d3",
}

fig = go.Figure()
for run_name, run_data in runs.items():
    cats_data = run_data["cats"]
    vals = [cats_data.get(c, {}).get("f1", 0) for c in categories]
    fig.add_trace(go.Bar(
        name=run_name, x=categories, y=vals,
        marker_color=colors_map[run_name],
        text=[f"{v:.0f}" for v in vals],
        textposition="outside",
    ))

fig.update_layout(
    barmode="group",
    yaxis_title="Token F1",
    yaxis_range=[0, 95],
    legend=dict(orientation="h", yanchor="bottom", y=1.02),
    template="plotly_white",
    height=450,
)
fig.show()
```

```{python}
#| label: fig-judge-categories
#| fig-cap: "LLM Judge score by category for SOTA (v6b) vs ceiling."

runs_j = {
    "v6b ‚ú¶ (SOTA)": versions["v6b ‚ú¶"],
    "Ceiling":      versions["Ceiling"],
}
fig = go.Figure()
for run_name, run_data in runs_j.items():
    cats_data = run_data["cats"]
    vals = [cats_data.get(c, {}).get("judge", 0) for c in categories]
    fig.add_trace(go.Bar(
        name=run_name, x=categories, y=vals,
        marker_color="#ffc107" if "SOTA" in run_name else "#d3d3d3",
        text=[f"{v:.0f}" for v in vals],
        textposition="outside",
    ))

fig.update_layout(
    barmode="group",
    yaxis_title="LLM Judge Score",
    yaxis_range=[0, 40],
    legend=dict(orientation="h", yanchor="bottom", y=1.02),
    template="plotly_white",
    height=400,
)
fig.show()
```

### Category observations

- **Procedures** had the largest single-category gain: **+27.8 F1** (v3‚Üív6b). Root cause was
  vague ordinal QA pairs ("first procedure", "second procedure") that were regenerated with
  named references, enabling proper retrieval.
- **Insights** and **bugs** are strong on judge (25.8 and 23.5) ‚Äî the content is retrievable
  and semantically correct even when token overlap is imperfect.
- **Patterns** is the weakest judge category (9.0) ‚Äî wording varies enough that semantic
  equivalence isn't captured by binary 0/1 scoring.
- **Decisions** and **solutions** sit close to ceiling ‚Äî low-hanging fruit is mostly captured.

---

## Retrieval Techniques

```{python}
#| label: fig-techniques
#| fig-cap: "Cumulative impact of each retrieval improvement on Token F1."

tech_labels = ["Dense\nEmbeddings", "+ HyDE", "+ Multi-\ncategory", "+ BM25\nHybrid", "+ Procedures\nQA fix"]
tech_f1     = [38.9, 45.0, 56.6, 58.5, 60.5]  # approximate cumulative

fig = go.Figure(go.Scatter(
    x=tech_labels, y=tech_f1,
    mode="lines+markers",
    fill="tozeroy",
    fillcolor="rgba(13,110,253,0.08)",
    line=dict(color="#0d6efd", width=3),
    marker=dict(size=10, color="#0d6efd"),
    text=[f"{v:.1f}" for v in tech_f1],
    textposition="top center",
))
fig.add_hline(y=ceiling_f1, line_dash="dash", line_color="#6c757d",
              annotation_text=f"Ceiling {ceiling_f1:.1f}")
fig.update_layout(
    yaxis_title="Token F1",
    yaxis_range=[30, 75],
    template="plotly_white",
    height=380,
)
fig.show()
```

### BM25 Hybrid Retrieval (RRF)

Dense embeddings excel at semantic similarity but miss exact technical terms.
BM25 catches exact matches that embeddings underweight. Reciprocal Rank Fusion combines both:

$$\text{RRF}(d) = \frac{1}{60 + r_\text{dense}(d)} + \frac{1}{60 + r_\text{bm25}(d)}$$

This reduced the **not-found rate from 18.4% ‚Üí 13.2%** ‚Äî rescuing 20 questions that
dense-only retrieval returned nothing for.

---

## Evaluation Methodology

| Aspect | Detail |
|--------|--------|
| **Dataset** | 386 Q&A pairs across 6 knowledge categories |
| **Retrieval** | top-k=12, threshold=0.15, hybrid BM25+dense |
| **F1 scoring** | Token overlap between predicted and reference answer |
| **Judge** | Gemini 2.5 Flash binary (0/1) correctness, √ó100 for % |
| **Ceiling** | Same questions answered with full knowledge context (no retrieval) |
| **Judge variance** | ~2.3 pts run-to-run due to binary 0/1 scoring; CI gate = 2.5 pts |

### What "exceeding the ceiling" means

The SOTA judge score (16.3) is **117%** of the ceiling (14.0). This happens because:

1. RAG retrieves *focused* context ‚Äî less noise than full-context injection
2. The LLM judge finds targeted answers easier to evaluate as correct
3. Full context can cause the model to hedge or add caveats that the judge penalises

---

## Remaining Gap & Next Steps

```{python}
#| label: fig-gap
#| fig-cap: "Gap to ceiling per category (F1). Negative = exceeds ceiling (SOTA already surpasses it)."

gap_f1 = []
gap_cats = []
for c in categories:
    ceil_val = versions["Ceiling"]["cats"].get(c, {}).get("f1", 0)
    sota_val = versions["v6b ‚ú¶"]["cats"].get(c, {}).get("f1", 0)
    gap_cats.append(c)
    gap_f1.append(sota_val - ceil_val)   # negative = gap remaining

fig = go.Figure(go.Bar(
    x=gap_cats, y=gap_f1,
    marker_color=["#198754" if g >= 0 else "#dc3545" for g in gap_f1],
    text=[f"{g:+.1f}" for g in gap_f1],
    textposition="outside",
))
fig.add_hline(y=0, line_color="#333", line_width=1)
fig.update_layout(
    yaxis_title="SOTA F1 ‚àí Ceiling F1",
    yaxis_range=[-20, 10],
    template="plotly_white",
    height=360,
)
fig.show()
```

- **Bugs** (‚àí11.2) and **patterns** (‚àí10.5) have the largest remaining F1 gaps
- **Decisions** has already surpassed its ceiling (+3.7) via focused retrieval
- **Procedures** effectively matches ceiling (74.7 vs 73.1) after QA regeneration

Likely next improvements: cross-encoder re-ranking, query expansion, and longer context windows
via `engram inject --lines 360` for the judge model.
