---
title: "engram Retrieval Evaluation"
subtitle: "Benchmarking persistent memory retrieval across knowledge categories"
date: today
---

```{python}
import json, pathlib, warnings
import pandas as pd
import plotly.graph_objects as go
import plotly.io as pio

pio.renderers.default = "notebook"
warnings.filterwarnings("ignore")

BASE = pathlib.Path("../../")

def load(name):
    p = BASE / "eval" / f"domain_results_{name}.json"
    d = json.loads(p.read_text())
    cats = d.get("by_category", {})
    total_n = d.get("total_questions") or sum(v.get("n", 0) for v in cats.values() if isinstance(v, dict))
    return dict(
        f1    = float(d.get("overall_f1") or d.get("f1", 0)),
        judge = float(d.get("overall_judge") or d.get("judge_score", 0)),
        nfr   = d.get("not_found_rate"),
        n     = total_n,
        cats  = cats,
    )

V = {
    "Baseline":  load("baseline"),
    "v1":        load("v1"),
    "v2":        load("v2"),
    "v3":        load("v3"),
    "v3-full":   load("v3_full"),
    "v4":        load("v4"),
    "v5":        load("v5"),
    "v6":        load("v6"),
    "v6b":       load("v6b"),
    "Ceiling":   load("ceiling"),
}
CATS = ["decisions", "solutions", "patterns", "bugs", "insights", "procedures"]
PALETTE = ["#16213e","#0f3460","#533483","#e94560","#f7c59f","#a8c0d6"]
```

::: {.hero}
::: {.hero-left}
## Evaluation summary

386 domain questions across 6 knowledge categories.
Two metrics: **Token F1** (exact lexical overlap) and
**LLM Judge** (binary semantic correctness via Gemini 2.5 Flash).
Ceiling = full-context injection upper bound (no retrieval step).
:::
::: {.hero-right}

```{python}
from IPython.display import HTML
v = V["v6b"]
c = V["Ceiling"]
cells = [
    ("60.5", "Token F1"),
    ("16.3", "LLM Judge"),
    (f"{v['f1']/c['f1']*100:.0f}%", "of F1 ceiling"),
    (f"{v['judge']/c['judge']*100:.0f}%", "of Judge ceiling"),
    (f"{v['nfr']:.0f}%", "Not-found rate"),
]
inner = "".join(
    f'<div class="kpi{"  accent" if i < 2 else ""}">'
    f'<div class="kpi-val">{val}</div>'
    f'<div class="kpi-lbl">{lbl}</div></div>'
    for i,(val,lbl) in enumerate(cells)
)
HTML(inner)
```

:::
:::

---

## Dataset & metrics

The evaluation dataset contains **386 question-answer pairs** hand-authored across six knowledge categories that mirror engram's storage taxonomy:

| Category | Questions | What is tested |
|---|---|---|
| decisions | 72 | Architectural choices, rationale captured in memory |
| solutions | 83 | Bug fixes and workarounds retrieved accurately |
| patterns | 67 | Code conventions recalled with correct detail |
| bugs | 51 | Known issues surfaced without hallucination |
| insights | 62 | Non-obvious findings retrieved in context |
| procedures | 51 | Step-by-step workflows recalled in order |

**Token F1** treats the answer as a bag of tokens and measures precision/recall against the reference. It rewards exact term overlap and is deterministic.
**LLM Judge** uses Gemini 2.5 Flash to score each answer 0 or 1 (semantically correct or not), multiplied by 100. Because it is binary, it has ~2–3 pt natural run-to-run variance — the CI regression gate is set at 2.5 pts to avoid false alarms.

The **ceiling** answers every question with the full knowledge base injected as context (no retrieval). It represents the upper bound for a perfect retriever with unlimited context.

---

## Results across versions

```{python}
labels = list(V.keys())
f1s    = [v["f1"]    for v in V.values()]
judges = [v["judge"] for v in V.values()]
cf1, cj = V["Ceiling"]["f1"], V["Ceiling"]["judge"]

fig = go.Figure()

# ceiling bands
fig.add_hrect(y0=cf1-1, y1=cf1+1, fillcolor="#e0e4f0", opacity=0.4, line_width=0)
fig.add_hrect(y0=cj-1,  y1=cj+1,  fillcolor="#f0e8e0", opacity=0.4, line_width=0)

fig.add_hline(y=cf1, line_dash="dash", line_color="#8899bb", line_width=1.5,
              annotation_text=f"F1 ceiling {cf1:.1f}", annotation_font_size=11)
fig.add_hline(y=cj,  line_dash="dot",  line_color="#bb9988", line_width=1.5,
              annotation_text=f"Judge ceiling {cj:.1f}", annotation_font_size=11,
              annotation_position="bottom right")

sizes  = [13 if l in ("v6b","Baseline") else 9 for l in labels]
syms_f = ["star" if l == "v6b" else "circle" for l in labels]

fig.add_trace(go.Scatter(
    x=labels, y=f1s, name="Token F1",
    mode="lines+markers",
    line=dict(color="#0f3460", width=2.5),
    marker=dict(size=sizes, symbol=syms_f, color="#0f3460",
                line=dict(color="#fff", width=1.5)),
))
fig.add_trace(go.Scatter(
    x=labels, y=judges, name="LLM Judge",
    mode="lines+markers",
    line=dict(color="#e94560", width=2.5),
    marker=dict(size=sizes, symbol=syms_f, color="#e94560",
                line=dict(color="#fff", width=1.5)),
))

fig.update_layout(
    height=400, template="plotly_white",
    yaxis=dict(title="Score", range=[0, 80]),
    xaxis_title="Version",
    legend=dict(orientation="h", y=1.12, x=0),
    margin=dict(t=50, r=120),
    hovermode="x unified",
)
fig.show()
```

The shaded bands around each ceiling show ±1 pt, roughly the natural measurement noise. Versions v6 and v6b cross the judge ceiling band — this is expected and explained in the [methodology section](#methodology-notes).

---

## What changed at each version

```{python}
prev = [None] + f1s[:-1]
deltas = [round(f - p, 1) if p else 0 for f, p in zip(f1s, prev)]
colors = ["#1a6b35" if d > 0.5 else "#9b1c1c" if d < -0.5 else "#888" for d in deltas]

fig = go.Figure(go.Bar(
    x=labels[1:], y=deltas[1:],
    marker_color=colors[1:],
    text=[f"{d:+.1f}" for d in deltas[1:]],
    textposition="outside",
    width=0.55,
))
fig.update_layout(
    height=320, template="plotly_white",
    yaxis=dict(title="ΔF1 vs previous version", range=[-4, 18], zeroline=True, zerolinecolor="#ccc"),
    xaxis_title="",
    margin=dict(t=20, b=40),
)
fig.show()
```

::: {.timeline}

<li>
<div class="tl-dot"></div>
<div class="tl-body">
<h4>v1 → v2 &nbsp;<span class="tl-delta neg">−0.1</span></h4>
<p>Output formatting cleanup and smoke-test tuning. No retrieval change — confirms the baseline retrieval quality was already the bottleneck, not output formatting.</p>
</div>
</li>

<li>
<div class="tl-dot big"></div>
<div class="tl-body">
<h4>v2 → v3 &nbsp;<span class="tl-delta pos">+14.9</span></h4>
<p><strong>HyDE (Hypothetical Document Embedding)</strong> — instead of embedding the raw question, a hypothetical answer is generated and embedded. This bridges the vocabulary gap between a terse query and a verbose knowledge entry. Simultaneously introduced multi-category extraction, letting each conversation contribute to decisions, solutions, patterns, bugs, insights, and procedures simultaneously rather than a single bucket.</p>
</div>
</li>

<li>
<div class="tl-dot"></div>
<div class="tl-body">
<h4>v3 → v3-full &nbsp;<span class="tl-delta pos">+2.9</span></h4>
<p>Scaled from a smoke-test subset (52 questions) to the full 385-question dataset. The gain reflects better category coverage; procedures in particular had been underweighted in the smoke set.</p>
</div>
</li>

<li>
<div class="tl-dot"></div>
<div class="tl-body">
<h4>v3-full → v4 &nbsp;<span class="tl-delta pos">+0.6</span></h4>
<p>Post-merge housekeeping: clippy fixes, rustfmt, no retrieval changes. Small gain from cleaner extraction output.</p>
</div>
</li>

<li>
<div class="tl-dot neg" style="background:#888; box-shadow:0 0 0 2px #888"></div>
<div class="tl-body">
<h4>v4 → v5 &nbsp;<span class="tl-delta neg">−0.6</span></h4>
<p>Noise run — no code changes. Demonstrates the ~2–3 pt natural judge variance across identical runs. The F1 drop is within measurement error for the dataset size.</p>
</div>
</li>

<li>
<div class="tl-dot big"></div>
<div class="tl-body">
<h4>v5 → v6 &nbsp;<span class="tl-delta pos">+3.3</span></h4>
<p>Three simultaneous improvements: <strong>(1) BM25 hybrid retrieval with RRF</strong> rescues exact-match queries that dense embeddings underweight; <strong>(2) procedures QA dataset regenerated</strong> — the original 50 questions used vague ordinal references ("the first procedure") that made retrieval impossible; regenerated with named references; <strong>(3) ADD artifact root cause fixed</strong> — the extraction LLM was occasionally prefixing entries with resolver keywords (ADD/NOOP), polluting knowledge files.</p>
</div>
</li>

<li>
<div class="tl-dot"></div>
<div class="tl-body">
<h4>v6 → v6b &nbsp;<span class="tl-delta pos">+0.6</span></h4>
<p>Independent confirmation run. Confirms v6 results are reproducible within expected variance.</p>
</div>
</li>

:::

---

## Category deep-dive

```{python}
fig = go.Figure()
run_cfg = [
    ("Baseline", "#d0d4de", "dot"),
    ("v3-full",  "#6ea8fe", "dash"),
    ("v6b",      "#e94560", "solid"),
    ("Ceiling",  "#aab4c8", "longdash"),
]
for run_name, color, dash in run_cfg:
    vals = [V[run_name]["cats"].get(c, {}).get("f1", 0) for c in CATS]
    fig.add_trace(go.Scatterpolar(
        r=vals + [vals[0]],
        theta=CATS + [CATS[0]],
        name=run_name,
        line=dict(color=color, width=2.5, dash=dash),
        fill="toself" if run_name == "v6b" else "none",
        fillcolor="rgba(233,69,96,0.06)" if run_name == "v6b" else None,
    ))

fig.update_layout(
    polar=dict(radialaxis=dict(visible=True, range=[0, 85])),
    height=440, template="plotly_white",
    legend=dict(orientation="h", y=-0.15, x=0.5, xanchor="center"),
    margin=dict(t=30),
)
fig.show()
```

```{python}
fig = go.Figure()
for i, (run_name, color, _) in enumerate(run_cfg):
    vals = [V[run_name]["cats"].get(c, {}).get("f1", 0) for c in CATS]
    fig.add_trace(go.Bar(
        name=run_name, x=CATS, y=vals,
        marker_color=color,
        text=[f"{v:.0f}" for v in vals],
        textposition="outside",
    ))

fig.update_layout(
    barmode="group", height=400, template="plotly_white",
    yaxis=dict(title="Token F1", range=[0, 95]),
    xaxis_title="",
    legend=dict(orientation="h", y=1.08),
    margin=dict(t=50),
)
fig.show()
```

### What the categories tell us

**Procedures** shows the most dramatic improvement (+27.8 F1, baseline→v6b). The gain came almost entirely from fixing the QA dataset: the original questions referenced procedures by ordinal position ("the third procedure") rather than by name, making retrieval a lottery. Once regenerated with named references, the retriever could anchor on distinctive phrases.

**Decisions** and **solutions** are closest to ceiling, suggesting the retrieval quality for well-structured factual entries is largely solved at the current corpus size.

**Bugs** and **patterns** have the largest remaining F1 gaps (−11 and −10 below ceiling). Bug reports tend to have diverse phrasing with technical identifiers; patterns are often abstract descriptions where synonym variation defeats token F1 even when the semantic answer is correct.

**Insights** scores highest on the judge metric (25.8) despite a moderate F1 (50.3) — the LLM judge finds the answers semantically correct even when the exact wording differs. This suggests F1 undervalues retrieval quality for this category.

---

## Retrieval methodology

### Dense embeddings + HyDE

Each knowledge entry is embedded using a sentence embedding model. At query time, instead of embedding the raw question, the system generates a *hypothetical answer* — a plausible but fabricated response — and embeds that. The intuition: a hypothetical answer lives in the same vector space as real answers, bridging the vocabulary gap between a short question and a verbose knowledge entry.

```
Q: "what did we decide about async runtimes?"
Hypothetical: "We decided to use tokio for all async operations because..."
Embed(hypothetical) → cosine search → top-k entries → answer
```

### BM25 hybrid with Reciprocal Rank Fusion

Dense retrieval excels at semantic similarity but misses exact technical terms — version numbers, function names, error codes. BM25 is purely lexical and catches these. The two ranked lists are fused with RRF:

$$\text{score}(d) = \frac{1}{60 + r_{\text{dense}}(d)} + \frac{1}{60 + r_{\text{BM25}}(d)}$$

The constant 60 dampens the influence of exact rank position, making the fusion robust to small rank differences at the top. BM25 uses $k_1 = 1.5$, $b = 0.75$ (standard defaults).

**Impact:** not-found rate dropped from 18.4% → 13.2% — 20 additional questions now return an answer where dense-only returned nothing.

```{python}
# Not-found rate comparison
versions_nfr = {k: v for k, v in V.items() if v["nfr"] is not None and k != "Ceiling"}
labels_nfr = list(versions_nfr.keys())
nfrs = [v["nfr"] for v in versions_nfr.values()]
colors_nfr = ["#e94560" if n < 15 else "#0f3460" for n in nfrs]

fig = go.Figure(go.Bar(
    x=labels_nfr, y=nfrs,
    marker_color=colors_nfr,
    text=[f"{n:.1f}%" for n in nfrs],
    textposition="outside",
    width=0.5,
))
fig.update_layout(
    height=300, template="plotly_white",
    yaxis=dict(title="Not-found rate (%)", range=[0, 25]),
    xaxis_title="",
    margin=dict(t=20, b=40),
    title=dict(text="Questions with no retrieved answer", font_size=13),
)
fig.show()
```

---

## Full results table

```{python}
rows = []
for name, v in V.items():
    pf1 = f"{v['f1']/V['Ceiling']['f1']*100:.0f}%" if name != "Ceiling" else "—"
    pj  = f"{v['judge']/V['Ceiling']['judge']*100:.0f}%" if name != "Ceiling" else "—"
    nfr = f"{v['nfr']:.1f}%" if v["nfr"] is not None else "—"
    rows.append({
        "Version":    name,
        "Token F1":   f"{v['f1']:.1f}",
        "LLM Judge":  f"{v['judge']:.1f}",
        "% F1 ceil":  pf1,
        "% J ceil":   pj,
        "Not-found":  nfr,
        "N":          str(int(v["n"])) if v["n"] else "—",
    })

html_rows = ""
for r in rows:
    cls = ' class="highlight"' if r["Version"] == "v6b" else \
          ' style="color:#aaa;font-style:italic"' if r["Version"] == "Ceiling" else ""
    cells = "".join(f"<td>{v}</td>" for v in r.values())
    html_rows += f"<tr{cls}>{cells}</tr>"

header = "".join(f"<th>{h}</th>" for h in rows[0].keys())
HTML(f"<table><thead><tr>{header}</tr></thead><tbody>{html_rows}</tbody></table>")
```

---

## Methodology notes

**Why judge can exceed ceiling.** The ceiling is computed by injecting the *entire* knowledge base as context. With thousands of tokens of mixed content, the model has to locate and extract the relevant fragment. RAG retrieves a small, focused set of entries — the model sees less noise and tends to give cleaner, more confident answers that the judge scores higher.

**Judge variance.** Binary scoring (0/1 per question, ×100) means each question is worth `100/N` ≈ 0.26 pts. A swing of ±10 questions changes the judge score by ±2.6 pts. Two identical runs can therefore differ by 2–3 pts with no code changes. The CI regression gate is set at 2.5 pts to account for this.

**Token F1 limitations.** F1 rewards token overlap but penalises paraphrases. A correct answer that uses synonyms scores lower than a partially wrong answer that reproduces the reference verbatim. This is why judge and F1 sometimes diverge sharply (e.g. insights: F1=50, judge=26 → the answers are semantically right but worded differently).

---

## What to improve next

```{python}
gap = {c: V["v6b"]["cats"].get(c,{}).get("f1",0) - V["Ceiling"]["cats"].get(c,{}).get("f1",0)
       for c in CATS}

fig = go.Figure(go.Bar(
    x=list(gap.keys()),
    y=list(gap.values()),
    marker_color=["#1a6b35" if v >= 0 else "#9b1c1c" for v in gap.values()],
    text=[f"{v:+.1f}" for v in gap.values()],
    textposition="outside",
    width=0.5,
))
fig.add_hline(y=0, line_color="#333", line_width=1)
fig.update_layout(
    height=320, template="plotly_white",
    yaxis=dict(title="v6b F1 − Ceiling F1", range=[-16, 8]),
    xaxis_title="",
    title=dict(text="Remaining gap to ceiling per category (positive = surpassed)", font_size=13),
    margin=dict(t=50),
)
fig.show()
```

- **Bugs and patterns** have the largest gaps (~11 pts each). Both suffer from high linguistic variation. Cross-encoder re-ranking — running a lightweight classifier over retrieved candidates — would help here by scoring semantic relevance rather than surface form.
- **Query expansion** (generating multiple query variants and merging results) could help for short, ambiguous questions in the bugs category.
- **Longer retrieval context** via `engram inject --lines 360` allows the answering model to see more candidates before committing to a response, at the cost of increased latency.
