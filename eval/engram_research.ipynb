{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# engram Memory System — Research Notebook\n",
    "\n",
    "This notebook documents the evaluation methodology, results, and improvement trajectory\n",
    "for the engram AI memory system.\n",
    "\n",
    "## Key metrics\n",
    "- **Token-F1**: Lexical overlap between predicted and gold answers\n",
    "- **LLM-judge**: Semantic correctness scored by gemini-2.5-flash (YES/NO)\n",
    "- **Not-found rate**: % questions where engram returned nothing\n",
    "- **Ceiling efficiency**: % of full-context (28K token) baseline we achieve\n",
    "\n",
    "## Eval datasets\n",
    "1. **Domain eval** (`eval/qa_dataset.json`): 385 QA pairs generated from engram's own knowledge.\n",
    "   Tests the full pipeline on its actual domain (Claude Code sessions).\n",
    "2. **LoCoMo-10** (`eval/locomo10.json`): Official long-term conversational memory benchmark.\n",
    "   10 multi-session conversations, 1986 QA pairs across 5 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path('..')  # repo root from eval/\n",
    "EVAL_DIR = Path('.')\n",
    "\n",
    "def load(path):\n",
    "    p = EVAL_DIR / path\n",
    "    if p.exists():\n",
    "        return json.loads(p.read_text())\n",
    "    return {}\n",
    "\n",
    "print('Notebook ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Domain Eval Results — Improvement Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all domain eval results\n",
    "runs = [\n",
    "    ('v1 baseline\\n(raw add, t=0.4)', load('domain_results_v1.json')),\n",
    "    ('v2 improvements\\n(HyDE, t=0.15)', load('domain_results_v3.json')),\n",
    "    ('v3 full 385Q\\n(HyDE+index, t=0.2)', load('domain_results_v3_full.json')),\n",
    "    ('Ceiling\\n(full context)', load('domain_results_ceiling.json')),\n",
    "]\n",
    "\n",
    "labels = [r[0] for r in runs if r[1]]\n",
    "f1_scores = [r[1].get('overall_f1', 0) for r in runs if r[1]]\n",
    "judge_scores = [r[1].get('overall_judge', 0) for r in runs if r[1]]\n",
    "not_found = [r[1].get('not_found_rate', 0) for r in runs if r[1]]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('engram Domain Eval — Improvement Trajectory', fontsize=14, fontweight='bold')\n",
    "\n",
    "colors = ['#e74c3c', '#f39c12', '#3498db', '#2ecc71']\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "# F1\n",
    "axes[0].bar(x, f1_scores, color=colors[:len(labels)], alpha=0.85, edgecolor='white')\n",
    "axes[0].axhline(67.9, color='green', linestyle='--', alpha=0.5, label='ceiling')\n",
    "axes[0].set_title('Token-F1')\n",
    "axes[0].set_xticks(x); axes[0].set_xticklabels(labels, fontsize=8)\n",
    "axes[0].set_ylim(0, 80)\n",
    "for i, v in enumerate(f1_scores): axes[0].text(i, v+1, f'{v:.1f}', ha='center', fontsize=9, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Judge\n",
    "axes[1].bar(x, judge_scores, color=colors[:len(labels)], alpha=0.85, edgecolor='white')\n",
    "axes[1].axhline(14.0, color='green', linestyle='--', alpha=0.5, label='ceiling')\n",
    "axes[1].axhline(67.13, color='orange', linestyle='--', alpha=0.5, label='Mem0 SOTA')\n",
    "axes[1].set_title('LLM-Judge Score')\n",
    "axes[1].set_xticks(x); axes[1].set_xticklabels(labels, fontsize=8)\n",
    "axes[1].set_ylim(0, 80)\n",
    "for i, v in enumerate(judge_scores): axes[1].text(i, v+0.3, f'{v:.1f}', ha='center', fontsize=9, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "# Not-found\n",
    "axes[2].bar(x, not_found, color=colors[:len(labels)], alpha=0.85, edgecolor='white')\n",
    "axes[2].axhline(6.5, color='green', linestyle='--', alpha=0.5, label='ceiling (6.5%)')\n",
    "axes[2].set_title('Not-Found Rate %')\n",
    "axes[2].set_xticks(x); axes[2].set_xticklabels(labels, fontsize=8)\n",
    "for i, v in enumerate(not_found): axes[2].text(i, v+0.5, f'{v:.1f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('domain_eval_trajectory.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'F1 improvement:    {f1_scores[0]:.1f} → {f1_scores[-2]:.1f}  (+{f1_scores[-2]-f1_scores[0]:.1f})')\n",
    "print(f'Judge improvement: {judge_scores[0]:.1f} → {judge_scores[-2]:.1f}  (+{judge_scores[-2]-judge_scores[0]:.1f})')\n",
    "print(f'Not-found:         {not_found[0]:.1f}% → {not_found[-2]:.1f}%  (-{not_found[0]-not_found[-2]:.1f}pp)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Per-Category Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['decisions', 'solutions', 'patterns', 'bugs', 'insights', 'procedures']\n",
    "\n",
    "baseline = load('domain_results_v1.json').get('by_category', {})\n",
    "current  = load('domain_results_v3_full.json').get('by_category', {})\n",
    "ceiling  = load('domain_results_ceiling.json').get('by_category', {})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Per-Category: Baseline vs Current vs Ceiling', fontsize=13, fontweight='bold')\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "w = 0.28\n",
    "\n",
    "for ax, metric, title in [\n",
    "    (axes[0], 'f1', 'Token-F1 by Category'),\n",
    "    (axes[1], 'judge', 'LLM-Judge by Category'),\n",
    "]:\n",
    "    v_base = [baseline.get(c, {}).get(metric, 0) for c in categories]\n",
    "    v_curr = [current.get(c, {}).get(metric, 0) for c in categories]\n",
    "    v_ceil = [ceiling.get(c, {}).get(metric, 0) for c in categories]\n",
    "\n",
    "    ax.bar(x - w, v_base, w, label='Baseline', color='#e74c3c', alpha=0.8)\n",
    "    ax.bar(x,     v_curr, w, label='Current',  color='#3498db', alpha=0.8)\n",
    "    ax.bar(x + w, v_ceil, w, label='Ceiling',  color='#2ecc71', alpha=0.8)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(categories, rotation=20, ha='right', fontsize=9)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_ylim(0, 75)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('category_breakdown.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LoCoMo Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoCoMo results collected from background runs\n",
    "locomo_versions = {\n",
    "    'v1\\n(raw add)': {'f1': 18.0, 'judge': None, 'by_cat': {\n",
    "        'single_hop': 19.1, 'temporal': 5.2, 'open_ended': 6.0,\n",
    "        'multi_hop': 23.9, 'multi_hop_cs': 9.1\n",
    "    }},\n",
    "    'v2\\n(fact+concise)': {'f1': 35.6, 'judge': None, 'by_cat': {\n",
    "        'single_hop': 30.5, 'temporal': 17.3, 'open_ended': 3.2,\n",
    "        'multi_hop': 48.1, 'multi_hop_cs': 0.0\n",
    "    }},\n",
    "    'v3\\n(2.5-pro+atomic)': {'f1': 45.4, 'judge': 49.5, 'by_cat': {\n",
    "        'single_hop': None, 'temporal': None, 'open_ended': None,\n",
    "        'multi_hop': None, 'multi_hop_cs': None\n",
    "    }},\n",
    "}\n",
    "\n",
    "# Baselines from literature\n",
    "baselines = {\n",
    "    'GPT-4\\n(no memory)': {'f1': 32.1, 'judge': None},\n",
    "    'Mem0\\n(token F1)':   {'f1': 38.72, 'judge': None},\n",
    "    'Mem0\\n(LLM judge)':  {'f1': None, 'judge': 67.13},\n",
    "    'Human\\nceiling':     {'f1': 87.9, 'judge': None},\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('LoCoMo Benchmark — engram vs Baselines', fontsize=13, fontweight='bold')\n",
    "\n",
    "# F1 comparison\n",
    "ax = axes[0]\n",
    "all_f1 = {**{k: v['f1'] for k, v in locomo_versions.items() if v['f1']},\n",
    "          **{k: v['f1'] for k, v in baselines.items() if v['f1']}}\n",
    "sorted_f1 = sorted(all_f1.items(), key=lambda x: x[1])\n",
    "names = [x[0] for x in sorted_f1]\n",
    "vals = [x[1] for x in sorted_f1]\n",
    "colors_f1 = ['#3498db' if 'engram' not in n and 'v' not in n.lower() else '#e74c3c' for n in names]\n",
    "engram_colors = {'v1': '#e74c3c', 'v2': '#f39c12', 'v3': '#9b59b6'}\n",
    "bar_colors = []\n",
    "for n in names:\n",
    "    if 'v1' in n: bar_colors.append('#e74c3c')\n",
    "    elif 'v2' in n: bar_colors.append('#f39c12')\n",
    "    elif 'v3' in n: bar_colors.append('#9b59b6')\n",
    "    else: bar_colors.append('#95a5a6')\n",
    "\n",
    "bars = ax.barh(names, vals, color=bar_colors, alpha=0.85, edgecolor='white')\n",
    "ax.axvline(38.72, color='orange', linestyle='--', alpha=0.7, label='Mem0 token-F1')\n",
    "ax.set_title('Token-F1 (higher = better)')\n",
    "ax.set_xlabel('F1 Score')\n",
    "for bar, val in zip(bars, vals):\n",
    "    ax.text(val + 0.5, bar.get_y() + bar.get_height()/2, f'{val:.1f}',\n",
    "            va='center', fontsize=9, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# Judge comparison  \n",
    "ax2 = axes[1]\n",
    "all_judge = {k: v['judge'] for k, v in locomo_versions.items() if v.get('judge')}\n",
    "all_judge['Mem0\\n(LLM judge)'] = 67.13\n",
    "sorted_j = sorted(all_judge.items(), key=lambda x: x[1])\n",
    "jnames = [x[0] for x in sorted_j]\n",
    "jvals = [x[1] for x in sorted_j]\n",
    "jcolors = ['#9b59b6' if 'v3' in n else '#95a5a6' for n in jnames]\n",
    "jbars = ax2.barh(jnames, jvals, color=jcolors, alpha=0.85, edgecolor='white')\n",
    "ax2.axvline(67.13, color='orange', linestyle='--', alpha=0.7, label='Mem0 LLM-judge')\n",
    "ax2.set_title('LLM-as-Judge (higher = better)')\n",
    "ax2.set_xlabel('Judge Score %')\n",
    "for bar, val in zip(jbars, jvals):\n",
    "    ax2.text(val + 0.5, bar.get_y() + bar.get_height()/2, f'{val:.1f}',\n",
    "             va='center', fontsize=9, fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('locomo_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nKey comparison:')\n",
    "print(f'engram v3 token-F1:  ~45.4  vs  Mem0: 38.72  → +6.7 BEATS Mem0 token-F1')\n",
    "print(f'engram v3 LLM-judge: ~49.5  vs  Mem0: 67.13  → -17.6 gap to Mem0 headline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieval Gap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ceiling efficiency analysis\n",
    "ceiling_f1    = 67.9\n",
    "ceiling_judge = 14.0\n",
    "\n",
    "iterations = [\n",
    "    {'label': 'v1 baseline\\nt=0.4 k=5\\nno HyDE', 'f1': 38.9, 'judge': 8.8, 'not_found': 31.2},\n",
    "    {'label': 'v3 (90Q)\\nt=0.15 k=12\\n+HyDE+index', 'f1': 53.7, 'judge': 15.0, 'not_found': 15.0},\n",
    "    {'label': 'v3 full (385Q)\\nt=0.2 k=8\\n+HyDE+index', 'f1': 56.6, 'judge': 15.3, 'not_found': 18.4},\n",
    "    {'label': 'Ceiling\\nfull 28K ctx\\nno retrieval', 'f1': ceiling_f1, 'judge': ceiling_judge, 'not_found': 6.5},\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle('Retrieval Efficiency vs Ceiling', fontsize=13, fontweight='bold')\n",
    "\n",
    "x = np.arange(len(iterations))\n",
    "colors_iter = ['#e74c3c', '#f39c12', '#3498db', '#2ecc71']\n",
    "\n",
    "for ax, metric, title, ceil_val in [\n",
    "    (axes[0], 'f1', 'Token-F1', ceiling_f1),\n",
    "    (axes[1], 'judge', 'LLM-Judge', ceiling_judge),\n",
    "]:\n",
    "    vals = [it[metric] for it in iterations]\n",
    "    bars = ax.bar(x, vals, color=colors_iter, alpha=0.85, edgecolor='white')\n",
    "    ax.axhline(ceil_val, color='green', linestyle='--', lw=2, label=f'ceiling ({ceil_val})')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([it['label'] for it in iterations], fontsize=7.5)\n",
    "    ax.legend()\n",
    "    for bar, val in zip(bars, vals):\n",
    "        pct = val / ceil_val * 100\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, val + 0.2,\n",
    "                f'{val:.1f}\\n({pct:.0f}%)', ha='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ceiling_efficiency.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nEfficiency summary:')\n",
    "for it in iterations:\n",
    "    eff_f1 = it['f1'] / ceiling_f1 * 100\n",
    "    eff_j  = it['judge'] / ceiling_judge * 100\n",
    "    print(f'  {it[\"label\"].split(chr(10))[0]:<20} F1={eff_f1:.0f}%  judge={eff_j:.0f}%  not_found={it[\"not_found\"]:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Impact Analysis\n",
    "\n",
    "Which improvements contributed how much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    ('Update Resolver\\n(ADD/UPDATE/NOOP)', 'Reduces knowledge duplication,\\nimproves answer precision', 'indirect'),\n",
    "    ('FadeMem Decay\\n(strength field)', 'Old knowledge decays,\\nfresh knowledge surfaces first', 'indirect'),\n",
    "    ('HyDE for ask\\n(hypothetical doc embed)', 'Better semantic match\\nespecially for implicit queries', '+8-12 F1'),\n",
    "    ('Concise mode\\n(--concise flag)', 'Short answers = better\\ntoken-F1 match', '+18 F1'),\n",
    "    ('Fresh embedding index\\n(procedures 3→21 chunks)', 'Previously uncovered\\ncategory now retrievable', '+39 F1 procedures'),\n",
    "    ('threshold 0.4→0.15\\ntop-k 5→12', 'Wider retrieval net\\nnot-found 31%→18%', '+8 F1'),\n",
    "    ('Fact extraction\\n(2.5-pro + entity tags)', 'LoCoMo: atomic facts with\\n[Person:X][Date:Y] anchors', '+16 F1 LoCoMo'),\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "fig.suptitle('Feature Impact Map', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Rough impact scores for visualization\n",
    "impacts = [3, 3, 8, 18, 15, 8, 16]\n",
    "feature_labels = [f[0] for f in features]\n",
    "colors_feat = ['#95a5a6', '#95a5a6', '#3498db', '#e74c3c', '#f39c12', '#2ecc71', '#9b59b6']\n",
    "\n",
    "bars = ax.barh(feature_labels, impacts, color=colors_feat, alpha=0.85, edgecolor='white')\n",
    "ax.set_xlabel('Approximate F1 impact')\n",
    "ax.set_title('Estimated F1 contribution per feature')\n",
    "for bar, (feat, desc, impact) in zip(bars, features):\n",
    "    ax.text(bar.get_width() + 0.2, bar.get_y() + bar.get_height()/2,\n",
    "            impact, va='center', fontsize=8, color='gray')\n",
    "\n",
    "gray_patch = mpatches.Patch(color='#95a5a6', label='Architecture (indirect)')\n",
    "blue_patch  = mpatches.Patch(color='#3498db', label='Retrieval quality')\n",
    "red_patch   = mpatches.Patch(color='#e74c3c', label='Synthesis quality')\n",
    "orange_patch = mpatches.Patch(color='#f39c12', label='Index coverage')\n",
    "green_patch  = mpatches.Patch(color='#2ecc71', label='Threshold tuning')\n",
    "purple_patch = mpatches.Patch(color='#9b59b6', label='Domain extraction')\n",
    "ax.legend(handles=[gray_patch, blue_patch, red_patch, orange_patch, green_patch, purple_patch],\n",
    "          loc='lower right', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_impact.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. How to Run Evals\n",
    "\n",
    "```bash\n",
    "# One-time setup: generate QA dataset from real knowledge\n",
    "python eval/gen_qa_dataset.py --project Personal --output eval/qa_dataset.json\n",
    "\n",
    "# Run eval (saves timestamped results, CI-gated)\n",
    "bash eval/run_eval.sh\n",
    "\n",
    "# Compare all runs\n",
    "python eval/compare.py eval/results/*.json\n",
    "\n",
    "# Automated improvement loop\n",
    "python eval/improvement_loop.py --project Personal --target-judge 13.5 --max-per-cat 20\n",
    "\n",
    "# Full ceiling baseline (no retrieval, full context)\n",
    "python eval/engram_eval.py --project Personal --dataset eval/qa_dataset.json \\\n",
    "    --full-context --use-judge --output eval/domain_results_ceiling.json\n",
    "\n",
    "# LoCoMo benchmark\n",
    "python eval/locomo_eval.py --engram ./target/release/engram --use-judge --workers 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Gap Analysis & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''\n",
    "╔══════════════════════════════════════════════════════════════╗\n",
    "  Current State (Feb 2026)\n",
    "╠══════════════════════════════════════════════════════════════╣\n",
    "\n",
    "  Domain eval (385 QA pairs, engram's actual domain):\n",
    "    Token-F1:   56.6  (83% of ceiling)\n",
    "    LLM-judge:  15.3  (109% of ceiling — concise beats full-context!)\n",
    "    Not found:  18.4% (ceiling: 6.5%)\n",
    "\n",
    "  LoCoMo-10 (social conversations):\n",
    "    v3 token-F1:  ~45.4  (BEATS Mem0 token-F1 of 38.72)\n",
    "    v3 LLM-judge: ~49.5  (gap to Mem0 67.13: -17.6)\n",
    "\n",
    "╠══════════════════════════════════════════════════════════════╣\n",
    "  Remaining gaps:\n",
    "\n",
    "  1. Not-found 18.4% → 6.5% (ceiling)\n",
    "     Fix: better chunking, adaptive threshold per category\n",
    "\n",
    "  2. procedures judge 4.0% (procedural lists score poorly)\n",
    "     Fix: multi-item answer format, step-by-step eval metric\n",
    "\n",
    "  3. LoCoMo judge 49.5% vs Mem0 67.13%\n",
    "     Fix: entity graph retrieval, temporal-aware indexing\n",
    "\n",
    "  4. decisions judge 11.1% (complex multi-part decisions)\n",
    "     Fix: decompose decisions into atomic sub-facts\n",
    "\n",
    "╠══════════════════════════════════════════════════════════════╣\n",
    "  Run improvement_loop.py to automate the fix cycle.\n",
    "╚══════════════════════════════════════════════════════════════╝\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
